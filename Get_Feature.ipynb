{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# “神策杯”2018高校算法大师赛 有监督特征生成\n",
    "\n",
    "特征 | 解释\n",
    "---|---\n",
    "id | 候选关键词所属样本id\n",
    "tags | 候选关键词\n",
    "cixing | 词性\n",
    "tfidf |jieba库算出的tfidf值\n",
    "ske | 共现矩阵偏度，From 论文《大数据时代基于统计特征的情报关键词提取方法》\n",
    "occur_in_title | 是否出现在标题\n",
    "occur_in_first_sentence | 是否出现在正文第一句\n",
    "occur_in_last_sentence | 是否出现在正文最后一句\n",
    "occur_in_other_sentence | 是否出现在正文中间\n",
    "len_tags | 词长\n",
    "num_tags | 当前样本候选关键词个数\n",
    "num_sen | 当前样本句子数量\n",
    "classes | 聚类特征，doc2vec+Kmeans\n",
    "len_text | 标题+正文长度\n",
    "textrank | jieba库计算的textrank值\n",
    "word_count | 词频\n",
    "tf | 词频 / 总单词数\n",
    "num_head_words | 头词频，候选关键词在前1/4文本里面出现频次\n",
    "hf | 头词频 / 前1/4文本单词数\n",
    "pr | tf / tf.sum()\n",
    "has_num | 候选关键词是否包含数字\n",
    "has_eng | 候选关键词是否包含英文\n",
    "is_TV | 是否是《..》里面的词\n",
    "sim | 文本doc2vec与候选关键词word2vec的余弦相似度\n",
    "sim_euc | 文本doc2vec与候选关键词word2vec的欧氏距离\n",
    "mean_l2 | 关键词所在句子平均长度\n",
    "meaxl2 | 关键词所在句子最大长度\n",
    "min_l2 | 关键词所在句子最小长度\n",
    "min_pos | 关键词最早出现词位置\n",
    "diff_min_pos_bili | 词位置相关统计特诊\n",
    "diff_kurt_pos_bili | 词位置相关统计特诊\n",
    "diff_max_min_sen_pos | 关键词所在句子位置相关统计特征\n",
    "diff_var_sen_pos_bili | 关键词所在句子位置相关统计特征\n",
    "mean_sim_tags | 单词平均相似度，基于word2vec\n",
    "diff_mean_sim_tags | 单词平均相似度相关统计特征\n",
    "kurt_sim_tags_256 | 单词相似度峰度，基于另一word2vec模型\n",
    "diff_max_min_sim_tags_256 | 单词平均相似度相关统计特征\n",
    "var_gongxian | 共现矩阵方差\n",
    "kurt_gongxian | 共现矩阵峰度\n",
    "diff_min_gongxian | 共现矩阵相关统计特征\n",
    "cixing_\\*_num | 当前样本候选关键词词性\\*的数量\n",
    "cixing_\\*_bili | 当前样本候选关键词词性\\*的比例\n",
    "label | 标签\n",
    "\n",
    "- 参考论文\n",
    "\n",
    "[1] 常耀成, 张宇翔, 王红, 等. 特征驱动的关键词提取算法综述[J]. 软件学报, 2018, 7: 015.\n",
    "\n",
    "[2] 李跃鹏, 金翠, 及俊川. 基于 word2vec 的关键词提取算法[J]. 科研信息化技术与应用, 2015, 6(4): 54-59.\n",
    "\n",
    "[3] 罗繁明, 杨海深. 大数据时代基于统计特征的情报关键词提取方法[J]. 情报资料工作, 2013, 34(3): 19r20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "import math\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\bigzhao\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.873 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "## 搜狗+百度词典 深蓝词典转换\n",
    "jieba.load_userdict('./字典/明星.txt')\n",
    "jieba.load_userdict('./字典/实体名词.txt')\n",
    "jieba.load_userdict('./字典/歌手.txt')\n",
    "jieba.load_userdict('./字典/动漫.txt')\n",
    "jieba.load_userdict('./字典/电影.txt')\n",
    "jieba.load_userdict('./字典/电视剧.txt')\n",
    "jieba.load_userdict('./字典/流行歌.txt')\n",
    "jieba.load_userdict('./字典/创造101.txt')\n",
    "jieba.load_userdict('./字典/百度明星.txt')\n",
    "jieba.load_userdict('./字典/美食.txt')\n",
    "jieba.load_userdict('./字典/FIFA.txt')\n",
    "jieba.load_userdict('./字典/NBA.txt')\n",
    "jieba.load_userdict('./字典/网络流行新词.txt')\n",
    "jieba.load_userdict('./字典/显卡.txt')\n",
    "\n",
    "## 爬取漫漫看网站和百度热点上面的词条\n",
    "jieba.load_userdict('./字典/漫漫看_明星.txt')\n",
    "jieba.load_userdict('./字典/百度热点人物+手机+软件.txt')\n",
    "jieba.load_userdict('./字典/自定义词典.txt')\n",
    "\n",
    "## 实体名词抽取之后的结果 有一定的人工过滤 \n",
    "## origin_zimu 这个只是把英文的组织名过滤出来\n",
    "jieba.load_userdict('./字典/person.txt')\n",
    "jieba.load_userdict('./字典/origin_zimu.txt')\n",
    "\n",
    "## 第一个是所有《》里面出现的实体名词\n",
    "## 后者是本地测试集的关键词加上了 \n",
    "jieba.load_userdict('./字典/出现的作品名字.txt')\n",
    "jieba.load_userdict('./字典/val_keywords.txt')\n",
    "\n",
    "## 网上随便找的停用词合集\n",
    "jieba.analyse.set_stop_words('./stopword.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = pd.read_csv('all_docs.txt', sep='\\001', header=None)\n",
    "all_docs.columns = ['id', 'title', 'content']\n",
    "all_docs.fillna('', inplace=True)\n",
    "\n",
    "val = pd.read_csv('train_docs_keywords.txt', sep='\\t', header=None)\n",
    "val.columns = ['id', 'kw']\n",
    "val.kw = val.kw.apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清理\n",
    "在比赛的过程中，通过分析文本中还是存在挺多噪声字符的，例如```&amp;```等，在这里就直接将这些字符清洗了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs['title_cut'] = all_docs['title'].apply(lambda x:''.join(filter(lambda ch: ch not in ' \\t◆#%', x)))\n",
    "all_docs['content_cut'] = all_docs['content'].apply(lambda x:''.join(filter(lambda ch: ch not in ' \\t◆#%', x)))\n",
    "\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('&amp;', ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('&quot;', ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('&#34;', ' ', x))\n",
    "\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('&amp;', ' ', x))\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('&quot;', ' ', x))\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('&#34;', ' ', x))\n",
    "\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('&nbsp;', ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('&nbsp;', ' ', x))\n",
    "\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('&gt;', ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('&gt;', ' ', x))\n",
    "\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('&lt;', ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('&lt;', ' ', x))\n",
    "\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub('hr/', ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub('hr/', ' ', x))\n",
    "\n",
    "strinfo = re.compile('······')\n",
    "\n",
    "all_docs['content_cut'] = all_docs['content_cut'].apply(lambda x: re.sub(strinfo, ' ', x))\n",
    "all_docs['title_cut'] = all_docs['title_cut'].apply(lambda x: re.sub(strinfo, ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理之句子拆分\n",
    "- 目的是考虑词的位置，加大关键词在标题/首局/末句的词频（权重）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(x):\n",
    "    \"\"\"\n",
    "    Utility function to return a list of sentences.\n",
    "    @param text The text that must be split in to sentences.\n",
    "    \"\"\"\n",
    "    ret = pd.Series()\n",
    "\n",
    "    ret['id'] = x['id']\n",
    "    if x['content_cut'] == '' or len(x['content_cut']) < 2:\n",
    "        ret['first_sentence'] = ''\n",
    "        ret['other_sentence'] = ''\n",
    "        ret['last_sentence'] = ''\n",
    "        return ret\n",
    "\n",
    "    sentence_delimiters = re.compile(u'[。？！；!?]')\n",
    "    sentences =[i for i in sentence_delimiters.split(x['content_cut']) if i != '']\n",
    "    num_sen = len(sentences)\n",
    "\n",
    "    if num_sen == 1:\n",
    "        ret['first_sentence'] = sentences[0]\n",
    "        ret['other_sentence'] = ''\n",
    "        ret['last_sentence'] = sentences[0]\n",
    "    elif num_sen == 2:\n",
    "        ret['first_sentence'] = sentences[0]\n",
    "        ret['other_sentence'] = ''\n",
    "        ret['last_sentence'] = sentences[-1]\n",
    "    else:\n",
    "        ret['first_sentence'] = sentences[0]\n",
    "        ret['other_sentence'] = ''.join(sentences[1:-1])\n",
    "        ret['last_sentence'] = sentences[-1]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = all_docs.apply(split_sentences, axis=1)\n",
    "\n",
    "all_docs = pd.merge(all_docs, tmp, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出第一句的作品名字 《。。。》\n",
    "# 感谢豆腐的baseline，我们可以知道假如标题中存在《。。。》，那么里面的内容直接就是关键词\n",
    "\n",
    "all_docs['first_sentence_reg'] = all_docs['first_sentence'].apply(lambda x:re.findall(r\"《(.+?)》\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取《》里面的实体名词加进jieba词库并持久化\n",
    "\n",
    "# reg = []\n",
    "# for word in all_docs['content_cut'].apply(lambda x:re.findall(r\"《(.+?)》\",x)).values:\n",
    "#     reg += word\n",
    "\n",
    "# with open('出现的作品名字.txt', 'w', encoding='utf-8') as f:\n",
    "#     for word in set(reg):\n",
    "#         f.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 历史作品名字 作为下面的一个特征\n",
    "TV = []\n",
    "with open('./字典/出现的作品名字.txt', 'r', encoding='utf-8') as f:\n",
    "    for word in f.readlines():\n",
    "        TV.append(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 这是根据本数据集算的idf文件\n",
    "idf = {}\n",
    "with open('my_idf.txt', 'r', encoding='utf-8') as f:\n",
    "    for i in f.readlines():\n",
    "        if len(i.strip().split()) == 2:\n",
    "            v = i.strip().split()\n",
    "            idf[v[0]] = float(v[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线下评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df):\n",
    "    def get_score(x):\n",
    "        score = 0\n",
    "        if x['label1'] in x['kw']:\n",
    "            score += 0.5\n",
    "        if x['label2'] in x['kw']:\n",
    "            score += 0.5 \n",
    "        return score\n",
    "    \n",
    "    pred = df[df.id.isin(val.id)]\n",
    "    tmp  = pd.merge(pred, val, on='id', how='left')\n",
    "    tmp['score'] = tmp.apply(get_score, axis=1)\n",
    "    print('Score: ',tmp.score.sum())\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classes_doc2vec.npy 文件是先算DOC2VEC向量 然后用Kmeans简单聚成10类\n",
    "classes = np.load('classes_doc2vec.npy')\n",
    "\n",
    "all_docs['classes'] = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec word2vec 计算主题相似性\n",
    "将Doc2vec word2vec两个模型的向量长度定为一样长就可以直接计算余弦相似度、欧式距离等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec.load('doc2vec.model')\n",
    "\n",
    "word2vec_model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv= word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(vec1, vec2):\n",
    "    npvec1, npvec2 = np.array(vec1), np.array(vec2)\n",
    "    return npvec1.dot(npvec2)/(math.sqrt((npvec1**2).sum()) * math.sqrt((npvec2**2).sum()))\n",
    "\n",
    "\n",
    "def Euclidean(vec1, vec2):\n",
    "    npvec1, npvec2 = np.array(vec1), np.array(vec2)\n",
    "    return math.sqrt(((npvec1-npvec2)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 后面加大窗口和迭代又算了一次word2vec 模型 主要是用来算候选关键词之间的相似度\n",
    "\n",
    "word2vec_model_256 = Word2Vec.load('word2vec_iter10_sh_1_hs_1_win_10.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs['idx'] = all_docs.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_df(df, train=True):\n",
    "    res = []\n",
    "    for index in tqdm(df.index):\n",
    "        \n",
    "        x = df.loc[index]\n",
    "        # TF-IDF\n",
    "        first_sentence_reg = ' '.join(x['first_sentence_reg'])\n",
    "        ## 这里主要是提取jieba默认的tf-idf值 我这边jieba自带的idf文件效果比自己提取的要好 所以也顺便用来筛选候选词 \n",
    "        ## 假如把topK设置为None的话 数据量会增大10倍 但是非常难跑 没有验证过效果\n",
    "        ## PS：这里我稍微修改了一下jieba的源码 allowpPOS实际是不允许出现的词性 即allowPOS = NotAllowPOS\n",
    "        \n",
    "        text = 19*(x['title_cut']+'。')+ 3*(x['first_sentence']+'。') + 1*(x['other_sentence']+'。')+\\\n",
    "                3*(x['last_sentence']+ '。') + 7*first_sentence_reg\n",
    "        jieba_tags = jieba.analyse.extract_tags(sentence=text, topK=20, allowPOS=('r','m','d', 'p', 'q', 'ad', 'u', 'f'), withWeight=True,\\\n",
    "                                          withFlag=True)\n",
    "\n",
    "        tags = []\n",
    "        cixing = []\n",
    "        weight = []\n",
    "        for tag in jieba_tags:\n",
    "            tags.append(tag[0].word)\n",
    "            cixing.append(tag[0].flag)\n",
    "            weight.append(tag[1])\n",
    "\n",
    "        sentence_delimiters = re.compile(u'[。？！；!?]')\n",
    "        sentences =[i for i in sentence_delimiters.split(text) if i != '']\n",
    "        num_sen = len(sentences)\n",
    "\n",
    "        words = []\n",
    "        num_words = 0\n",
    "        for sen in sentences:\n",
    "            cut = jieba.lcut(sen)\n",
    "            words.append(cut)\n",
    "            num_words += len(cut)\n",
    "        \n",
    "        new_tags = []\n",
    "        new_cixing = []\n",
    "        new_weight = []\n",
    "        len_tags = []\n",
    "        for i in range(len(tags)):\n",
    "            if tags[i].isdigit() and tags[i] not in ['985', '211']:\n",
    "                continue\n",
    "            if ',' in tags[i]:\n",
    "                continue\n",
    "            new_tags.append(tags[i])\n",
    "            new_weight.append(weight[i])\n",
    "            new_cixing.append(cixing[i])\n",
    "            len_tags.append(len(tags[i]))\n",
    "            \n",
    "            \n",
    "        ## 位置特征： 1. 是否出现在标题 2.是否出现在第一句 3.是否出现在最后一句 4.出现在正文中间部分\n",
    "        occur_in_title = np.zeros(len(new_tags))\n",
    "        occur_in_first_sentence = np.zeros(len(new_tags))\n",
    "        occur_in_last_sentence = np.zeros(len(new_tags))\n",
    "        occur_in_other_sentence = np.zeros(len(new_tags))\n",
    "        for i in range(len(new_tags)):\n",
    "            if new_tags[i] in x['title_cut']:\n",
    "                occur_in_title[i] = 1\n",
    "            if new_tags[i] in x['first_sentence']:\n",
    "                occur_in_first_sentence[i] = 1\n",
    "            if new_tags[i] in x['last_sentence']:\n",
    "                occur_in_last_sentence[i] = 1\n",
    "            if new_tags[i] in x['other_sentence']:\n",
    "                occur_in_other_sentence[i] = 1\n",
    "        \n",
    "        \n",
    "        ## 共现矩阵及相关统计特征 这里我一开始统计了好多 例如均值、方差、偏度等 得到新特征后贪心验证只保留以下三个 下面的统计特征同理\n",
    "        num_tags = len(new_tags)\n",
    "        arr = np.zeros((num_tags, num_tags))\n",
    "        for i in range(num_tags):\n",
    "            for j in range(i+1, num_tags):\n",
    "                count = 0\n",
    "                for word in words:\n",
    "                    if new_tags[i] in word and new_tags[j] in word:\n",
    "                        count += 1\n",
    "                arr[i, j] = count\n",
    "                arr[j, i] = count\n",
    "        ske = stats.skew(arr)\n",
    "        # cols += ['var_gongxian']\n",
    "        # cols += ['kurt_gongxian']\n",
    "        # cols += ['diff_min_gongxian']   \n",
    "        var_gongxian = np.zeros(len(new_tags))\n",
    "        kurt_gongxian = np.zeros(len(new_tags))\n",
    "        diff_min_gongxian = np.zeros(len(new_tags))\n",
    "        for i in range(len(new_tags)):\n",
    "            var_gongxian[i] = np.var(arr[i])\n",
    "            kurt_gongxian[i] = stats.kurtosis(arr[i])\n",
    "            diff_sim = np.diff(arr[i])\n",
    "            if len(diff_sim) > 0:\n",
    "                diff_min_gongxian[i] = np.min(diff_sim)\n",
    "\n",
    "                \n",
    "        ## textrank特征\n",
    "        textrank_tags = dict(jieba.analyse.textrank(sentence=text, allowPOS=('r','m','d', 'p', 'q', 'ad', 'u', 'f'), withWeight=True))\n",
    "        \n",
    "        textrank = []\n",
    "        for tag in new_tags:\n",
    "            if tag in textrank_tags:\n",
    "                textrank.append(textrank_tags[tag])\n",
    "            else:\n",
    "                textrank.append(0)\n",
    "                \n",
    "        all_words = np.concatenate(words).tolist()\n",
    "        \n",
    "        ## 词频\n",
    "        tf = []\n",
    "        for tag in new_tags:\n",
    "            tf.append(all_words.count(tag))\n",
    "        tf = np.array(tf)\n",
    "        \n",
    "        ## hf: 头词频，文本内容前1/4候选词词频\n",
    "        hf = []\n",
    "        head = len(words) // 4 + 1\n",
    "        head_words = np.concatenate(words[:head]).tolist()\n",
    "        for tag in new_tags:\n",
    "            hf.append(head_words.count(tag))\n",
    "        \n",
    "        ## has_num：是否包含数字\n",
    "        ## has_eng: 是否包含字母\n",
    "        def hasNumbers(inputString):\n",
    "            return bool(re.search(r'\\d', inputString))\n",
    "        def hasEnglish(inputString):\n",
    "            return bool(re.search(r'[a-zA-Z]', inputString))\n",
    "        has_num = []\n",
    "        has_eng = []\n",
    "        for tag in new_tags:\n",
    "            if hasNumbers(tag):\n",
    "                has_num.append(1)\n",
    "            else:\n",
    "                has_num.append(0)\n",
    "            if hasEnglish(tag):\n",
    "                has_eng.append(1)\n",
    "            else:\n",
    "                has_eng.append(0)\n",
    "                \n",
    "        ## is_TV:是否为作品名称\n",
    "        is_TV = []\n",
    "        for tag in new_tags:\n",
    "            if tag in TV:\n",
    "                is_TV.append(1)\n",
    "            else:\n",
    "                is_TV.append(0)\n",
    "                \n",
    "        ## idf: 用训练集跑出的逆词频\n",
    "        v_idf = []\n",
    "        for tag in new_tags:\n",
    "            v_idf.append(idf.get(tag, 0))\n",
    "        \n",
    "        ## 计算文本相似度，这里直接用doc2vec跟每个单词的word2vec做比较\n",
    "        ## sim: 余弦相似度\n",
    "        ## sim_euc：欧氏距离\n",
    "        default = np.zeros(100)\n",
    "        doc_vec = doc2vec_model.docvecs.vectors_docs[x['idx']]\n",
    "        sim = []\n",
    "        sim_euc = []\n",
    "        for tag in new_tags:\n",
    "            if tag in wv:\n",
    "                sim.append(Cosine(wv[tag], doc_vec))\n",
    "                sim_euc.append(Euclidean(wv[tag], doc_vec))\n",
    "            else:\n",
    "                sim.append(Cosine(default, doc_vec))\n",
    "                sim_euc.append(Euclidean(default, doc_vec))\n",
    "                \n",
    "        ## 关键词所在句子长度 L2，记录为列表，然后算统计特征 \n",
    "        mean_l2 = np.zeros(len(new_tags))\n",
    "        max_l2 = np.zeros(len(new_tags))\n",
    "        min_l2 = np.zeros(len(new_tags))\n",
    "        for i in range(len(new_tags)):\n",
    "            tmp = []\n",
    "            for word in words:\n",
    "                if new_tags[i] in word:\n",
    "                    tmp.append(len(word))\n",
    "            if len(tmp) > 0:\n",
    "                mean_l2[i] = np.mean(tmp)\n",
    "                max_l2[i] = np.max(tmp)\n",
    "                min_l2[i] = np.min(tmp)\n",
    "                \n",
    "# cols += ['min_pos']\n",
    "# cols += ['diff_min_pos_bili']\n",
    "# cols += ['diff_kurt_pos_bili']  \n",
    "\n",
    "        ## 关键词所在位置，记录为列表，然后算统计特征 \n",
    "\n",
    "        min_pos = [np.NaN for _ in range(len(new_tags))]\n",
    "        diff_min_pos_bili = [np.NaN for _ in range(len(new_tags))]\n",
    "        diff_kurt_pos_bili = [np.NaN for _ in range(len(new_tags))]\n",
    "        \n",
    "        for i in range(len(new_tags)):\n",
    "            pos = [a for a in range(len(all_words)) if all_words[a] == new_tags[i]]\n",
    "            pos_bili = np.array(pos) / len(all_words)\n",
    "            \n",
    "            if len(pos) > 0:\n",
    "                min_pos[i] = np.min(pos)\n",
    "                diff_pos = np.diff(pos)\n",
    "                diff_pos_bili = np.diff(pos_bili)\n",
    "                if len(diff_pos) > 0:\n",
    "                    diff_min_pos_bili[i] = np.min(diff_pos_bili)\n",
    "                    diff_kurt_pos_bili[i] = stats.kurtosis(diff_pos_bili)\n",
    "                    \n",
    "        ## 关键词所在句子位置特征，也是做成列表，做统计特征\n",
    "        # cols += ['diff_max_min_sen_pos']\n",
    "        # cols += ['diff_var_sen_pos_bili']\n",
    "\n",
    "        diff_max_min_sen_pos =  [np.NaN for _ in range(len(new_tags))]\n",
    "        diff_var_sen_pos_bili =  [np.NaN for _ in range(len(new_tags))]  \n",
    "        for i in range(len(new_tags)):\n",
    "            pos = [a for a in range(len(words)) if new_tags[i] in words[a]]\n",
    "            pos_bili = np.array(pos) / len(all_words)\n",
    "            \n",
    "            if len(pos) > 0:\n",
    "                diff_pos = np.diff(pos)\n",
    "                diff_pos_bili = np.diff(pos_bili)\n",
    "                if len(diff_pos) > 0:\n",
    "                    diff_max_min_sen_pos[i] = np.max(diff_pos) - np.min(diff_pos)\n",
    "                    diff_var_sen_pos_bili[i] = np.var(diff_pos_bili)\n",
    "                    \n",
    "#         ## 左右信息熵 没用\n",
    "#         left_entropy = []\n",
    "#         right_entropy = []\n",
    "#         for tag in new_tags:\n",
    "#             left = []\n",
    "#             right = []\n",
    "#             for word in words:\n",
    "#                 if len(word) < 3:\n",
    "#                     continue\n",
    "#                 for i in range(len(word)):\n",
    "#                     if word[i] == tag:\n",
    "#                         if i < 1:\n",
    "#                             left.append('None')\n",
    "#                             right.append(word[i+1])\n",
    "#                         if i == (len(word) - 1):\n",
    "#                             left.append(word[i-1])\n",
    "#                             right.append('None')\n",
    "#             left_entropy.append(calc_ent(np.array(left)))\n",
    "#             right_entropy.append(calc_ent(np.array(right)))\n",
    "                \n",
    "        ## 候选关键词之间的相似度 word2vec gensim 窗口默认 迭代默认 向量长度100\n",
    "        ## sim_tags_arr：相似度矩阵\n",
    "        sim_tags_arr = np.zeros((len(new_tags), len(new_tags)))\n",
    "        for i in range(len(new_tags)):\n",
    "            for j in range(i+1, len(new_tags)):\n",
    "                if new_tags[i] in wv and new_tags[j] in wv:\n",
    "                    sim_tags_arr[i, j] = word2vec_model.similarity(new_tags[i], new_tags[j])\n",
    "                    sim_tags_arr[j, i] = sim_tags_arr[i, j]\n",
    "            # cols += ['mean_sim_tags']\n",
    "# cols += ['diff_mean_sim_tags']        \n",
    "#         max_sim_tags = np.zeros(len(new_tags))\n",
    "#         min_sim_tags = np.zeros(len(new_tags))\n",
    "        mean_sim_tags = np.zeros(len(new_tags))\n",
    "#         var_sim_tags = np.zeros(len(new_tags))\n",
    "#         skew_sim_tags = np.zeros(len(new_tags))\n",
    "#         kurt_sim_tags = np.zeros(len(new_tags))\n",
    "#         max_min_sim_tags = np.zeros(len(new_tags))\n",
    "#         diff_max_sim_tags = np.zeros(len(new_tags))\n",
    "#         diff_min_sim_tags = np.zeros(len(new_tags))\n",
    "        diff_mean_sim_tags = np.zeros(len(new_tags))\n",
    "#         diff_var_sim_tags = np.zeros(len(new_tags))\n",
    "#         diff_skew_sim_tags = np.zeros(len(new_tags))\n",
    "#         diff_kurt_sim_tags = np.zeros(len(new_tags))\n",
    "#         diff_max_min_sim_tags = np.zeros(len(new_tags))       \n",
    "        for i in range(len(new_tags)):\n",
    "#             max_sim_tags[i] = np.max(sim_tags_arr[i])\n",
    "#             min_sim_tags[i] = np.min(sim_tags_arr[i])\n",
    "            mean_sim_tags[i] = np.mean(sim_tags_arr[i])\n",
    "#             var_sim_tags[i] = np.var(sim_tags_arr[i])\n",
    "#             skew_sim_tags[i] = stats.skew(sim_tags_arr[i])\n",
    "#             kurt_sim_tags[i] = stats.kurtosis(sim_tags_arr[i])\n",
    "#             max_min_sim_tags[i] = np.max(sim_tags_arr[i]) - np.min(sim_tags_arr[i])\n",
    "            diff_sim = np.diff(sim_tags_arr[i])\n",
    "            if len(diff_sim) > 0:\n",
    "#                 diff_max_sim_tags[i] = np.max(diff_sim)\n",
    "#                 diff_min_sim_tags[i] = np.min(diff_sim)\n",
    "                diff_mean_sim_tags[i] = np.mean(diff_sim)\n",
    "#                 diff_var_sim_tags[i] = np.var(diff_sim)\n",
    "#                 diff_skew_sim_tags[i] = stats.skew(diff_sim)\n",
    "#                 diff_kurt_sim_tags[i] = stats.kurtosis(diff_sim)\n",
    "#                 diff_max_min_sim_tags[i] = np.max(diff_sim) - np.min(diff_sim)\n",
    "\n",
    "        ## 候选关键词之间的相似度 word2vec gensim 窗口10 迭代10 向量长度256 \n",
    "    \n",
    "        sim_tags_arr_255 = np.zeros((len(new_tags), len(new_tags)))\n",
    "        for i in range(len(new_tags)):\n",
    "            for j in range(i+1, len(new_tags)):\n",
    "                if new_tags[i] in word2vec_model_256 and new_tags[j] in word2vec_model_256:\n",
    "                    sim_tags_arr_255[i, j] = word2vec_model_256.similarity(new_tags[i], new_tags[j])\n",
    "                    sim_tags_arr_255[j, i] = sim_tags_arr_255[i, j]\n",
    "# cols += ['diff_max_min_sim_tags_256']\n",
    "# cols += ['kurt_sim_tags_256']\n",
    "\n",
    "        kurt_sim_tags_256 = np.zeros(len(new_tags))\n",
    "        diff_max_min_sim_tags_256 = np.zeros(len(new_tags))       \n",
    "        for i in range(len(new_tags)):\n",
    "            kurt_sim_tags_256[i] = stats.kurtosis(sim_tags_arr_255[i])\n",
    "            diff_sim = np.diff(sim_tags_arr_255[i])\n",
    "            if len(diff_sim) > 0:\n",
    "                diff_max_min_sim_tags_256[i] = np.max(diff_sim) - np.min(diff_sim)   \n",
    "        \n",
    "        \n",
    "        ## label 训练集打标签\n",
    "        if train:\n",
    "            label = []\n",
    "            for tag in new_tags:\n",
    "                if tag in x.kw:\n",
    "                    label.append(1)\n",
    "                else:\n",
    "                    label.append(0)\n",
    "                    \n",
    "        ## 不同词性的比例\n",
    "        cixing_counter = Counter(new_cixing)\n",
    "        \n",
    "        fea = pd.DataFrame()\n",
    "        fea['id'] = [x['id'] for _ in range(len(new_tags))]\n",
    "        fea['tags'] = new_tags\n",
    "        fea['cixing'] = new_cixing\n",
    "\n",
    "\n",
    "        fea['tfidf'] = new_weight\n",
    "        fea['ske'] = ske\n",
    "        \n",
    "        fea['occur_in_title'] = occur_in_title\n",
    "        fea['occur_in_first_sentence'] = occur_in_first_sentence\n",
    "        fea['occur_in_last_sentence'] = occur_in_last_sentence\n",
    "        fea['occur_in_other_sentence'] = occur_in_other_sentence\n",
    "        fea['len_tags'] = len_tags\n",
    "        fea['num_tags'] = num_tags\n",
    "        fea['num_words'] = num_words\n",
    "        fea['num_sen'] = num_sen\n",
    "        fea['classes'] = x['classes']\n",
    "\n",
    "        fea['len_text'] = len(x['title_cut'] + x['content_cut'])\n",
    "        fea['textrank'] = textrank\n",
    "        fea['word_count'] = tf\n",
    "        fea['tf'] = tf / num_words\n",
    "        fea['num_head_words'] = len(head_words)\n",
    "        fea['head_word_count'] = hf\n",
    "        fea['hf'] = np.array(hf) / len(head_words)\n",
    "        fea['pr'] = tf / tf.sum()\n",
    "        fea['has_num'] = has_num\n",
    "        fea['has_eng'] = has_eng\n",
    "        fea['is_TV'] = is_TV\n",
    "        fea['idf'] = v_idf\n",
    "        fea['sim'] = sim\n",
    "        fea['sim_euc'] = sim_euc\n",
    "\n",
    "        fea['mean_l2'] = mean_l2\n",
    "        fea['meaxl2'] = max_l2\n",
    "        fea['min_l2'] = min_l2\n",
    "        \n",
    "        fea['min_pos'] = min_pos\n",
    "        fea['diff_min_pos_bili'] = diff_min_pos_bili\n",
    "        fea['diff_kurt_pos_bili'] = diff_kurt_pos_bili\n",
    "    \n",
    "        fea['diff_max_min_sen_pos'] = diff_max_min_sen_pos\n",
    "        fea['diff_var_sen_pos_bili'] = diff_var_sen_pos_bili\n",
    "\n",
    "        fea['mean_sim_tags'] = mean_sim_tags\n",
    "        fea['diff_mean_sim_tags'] = diff_mean_sim_tags\n",
    "\n",
    "        fea['kurt_sim_tags_256'] = kurt_sim_tags_256\n",
    "        fea['diff_max_min_sim_tags_256'] = diff_max_min_sim_tags_256\n",
    "        fea['var_gongxian'] = var_gongxian\n",
    "        fea['kurt_gongxian'] = kurt_gongxian\n",
    "        fea['diff_min_gongxian'] = diff_min_gongxian\n",
    "        \n",
    "        ## 当前文本候选关键词词性比例\n",
    "        for c in ['x', 'nz', 'l', 'n', 'v', 'ns', 'j', 'a', 'vn', 'nr', 'eng', 'nrt',\n",
    "                  't', 'z', 'i', 'b', 'o', 'nt', 'vd', 'c', 's', 'nrfg', 'mq', 'rz',\n",
    "                  'e', 'y', 'an', 'rr']:\n",
    "            fea['cixing_{}_num'.format(c)] = cixing_counter[c]\n",
    "            fea['cixing_{}_bili'.format(c)] = cixing_counter[c] / len(new_cixing)\n",
    "\n",
    "        if train:\n",
    "            fea['label'] = label\n",
    "        res.append(fea)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = pd.merge(val, all_docs, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = get_train_df(train_doc, train=True)\n",
    "\n",
    "train_df = pd.concat(res, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/108295 [00:00<?, ?it/s]c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:240: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:280: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:281: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "\n",
      "  0%|          | 1/108295 [00:00<11:47:35,  2.55it/s]\n",
      "  0%|          | 2/108295 [00:00<7:48:42,  3.85it/s] \n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\anaconda3\\lib\\site-packages\\tqdm\\_monitor.py\", line 63, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"c:\\anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "  0%|          | 3/108295 [00:00<7:39:03,  3.93it/s]\n",
      "  0%|          | 4/108295 [00:00<6:50:20,  4.40it/s]c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      " 92%|█████████▏| 99251/108295 [6:06:12<33:22,  4.52it/s]  c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:331: RuntimeWarning: invalid value encountered in true_divide\n",
      "100%|██████████| 108295/108295 [6:49:32<00:00,  4.41it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = get_train_df(all_docs, train=False)\n",
    "\n",
    "test_df = pd.concat(res, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.to_csv('train_df_v7.csv', index=False)\n",
    "test_df = pd.to_csv('train_df_v7.csv', index=False)\n",
    "\n",
    "# train_df = pd.read_csv('train_df_v7.csv')\n",
    "# test_df = pd.read_csv('train_df_v7.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "nav_menu": {
    "height": "132px",
    "width": "166px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "539px",
    "left": "1px",
    "right": "20px",
    "top": "96px",
    "width": "156px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
